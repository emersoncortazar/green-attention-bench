{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a177b6f-bec6-4057-92f1-e3bf2b9015f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c325d8-467f-4263-86a5-fb9ac5e061d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\models\\\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = os.path.join(\n",
    "    \"..\", \"models\", \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    ")\n",
    "\n",
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ed8111-fe46-45ca-abd0-8fb15626bd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=2048,        # context length (keep modest for now)\n",
    "    n_threads=8,       # CPU threads (we'll tune later)\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fb2107e-3f64-4730-a1b1-561c218def82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explain in 5 bullet points what 'energy efficiency in AI inference' means.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Explain in 5 bullet points what 'energy efficiency in AI inference' means.\"\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59ca979b-ffed-44b8-807b-ef06769878ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer according to: Energy efficiency in AI inference is the process of optimizing the performance of AI models in terms of energy consumption, while maintaining a minimum level of performance. Energy efficiency in AI inference is closely related to energy-efficient computing, which is the process of reducing energy consumption in computing systems. Energy efficiency in AI inference refers to the optimization of AI inference hardware and software, including algorithms, data management, and inference algorithms. Energy efficiency in AI inference can be measured in different ways, depending on the requirements of the system. In some cases, energy efficiency can be measured by comparing the energy consumption of\n",
      "\n",
      "--- Metrics ---\n",
      "Tokens generated: 128\n",
      "Time elapsed (s): 10.06\n",
      "Tokens/sec: 12.73\n",
      "Memory delta (MB): 160.5\n"
     ]
    }
   ],
   "source": [
    "# Measure memory before\n",
    "process = psutil.Process()\n",
    "mem_before = process.memory_info().rss / 1e6  # MB\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=128,\n",
    "    temperature=0.7,\n",
    "    stop=[\"</s>\"]\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Measure memory after\n",
    "mem_after = process.memory_info().rss / 1e6  # MB\n",
    "\n",
    "generated_text = output[\"choices\"][0][\"text\"]\n",
    "num_tokens = output[\"usage\"][\"completion_tokens\"]\n",
    "elapsed = end_time - start_time\n",
    "tokens_per_sec = num_tokens / elapsed\n",
    "\n",
    "print(generated_text)\n",
    "print(\"\\n--- Metrics ---\")\n",
    "print(f\"Tokens generated: {num_tokens}\")\n",
    "print(f\"Time elapsed (s): {elapsed:.2f}\")\n",
    "print(f\"Tokens/sec: {tokens_per_sec:.2f}\")\n",
    "print(f\"Memory delta (MB): {mem_after - mem_before:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb17649-7efd-4526-8ada-44993ee48d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
