{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a177b6f-bec6-4057-92f1-e3bf2b9015f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c325d8-467f-4263-86a5-fb9ac5e061d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\models\\\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = os.path.join(\n",
    "    \"..\", \"models\", \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    ")\n",
    "\n",
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ed8111-fe46-45ca-abd0-8fb15626bd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=2048,        # context length (keep modest for now)\n",
    "    n_threads=8,       # CPU threads (we'll tune later)\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fb2107e-3f64-4730-a1b1-561c218def82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explain in 5 bullet points what 'energy efficiency in AI inference' means.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Explain in 5 bullet points what 'energy efficiency in AI inference' means.\"\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59ca979b-ffed-44b8-807b-ef06769878ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Explain how machine learning models are being improved to meet the needs of this new classification. Explain how energy efficiency in AI inference can be achieved by optimizing the design of machine learning models to minimize the energy usage of the inference process. Explain the benefits of using machine learning for energy efficiency in AI inference. Explain how machine learning can improve energy efficiency in AI inference by reducing the need for excessive energy consumption. Explain how machine learning can achieve better energy efficiency in AI inference by optimizing the machine learning model's design for energy efficiency.\n",
      "\n",
      "--- Metrics ---\n",
      "Tokens generated: 117\n",
      "Time elapsed (s): 10.06\n",
      "Tokens/sec: 11.63\n",
      "Memory delta (MB): 0.1\n"
     ]
    }
   ],
   "source": [
    "# Measure memory before\n",
    "process = psutil.Process()\n",
    "mem_before = process.memory_info().rss / 1e6  # MB\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=128,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Measure memory after\n",
    "mem_after = process.memory_info().rss / 1e6  # MB\n",
    "\n",
    "generated_text = output[\"choices\"][0][\"text\"]\n",
    "num_tokens = output[\"usage\"][\"completion_tokens\"]\n",
    "elapsed = end_time - start_time\n",
    "tokens_per_sec = num_tokens / elapsed\n",
    "\n",
    "print(generated_text)\n",
    "print(\"\\n--- Metrics ---\")\n",
    "print(f\"Tokens generated: {num_tokens}\")\n",
    "print(f\"Time elapsed (s): {elapsed:.2f}\")\n",
    "print(f\"Tokens/sec: {tokens_per_sec:.2f}\")\n",
    "print(f\"Memory delta (MB): {mem_after - mem_before:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb17649-7efd-4526-8ada-44993ee48d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
