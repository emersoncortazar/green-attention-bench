{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ae0f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bf360e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\models\\\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = os.path.join(\n",
    "    \"..\", \"models\", \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    ")\n",
    "\n",
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "877bae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process()\n",
    "mem_before = process.memory_info().rss / 1e6  # MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa0b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) > n_ctx_train (2048) -- possible training context overflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=8192,\n",
    "    n_threads=8,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dccf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_after = process.memory_info().rss / 1e6  # MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a46a19fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory delta (model footprint) (MB): -156.9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory delta (model footprint) (MB): {mem_after - mem_before:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386a856-06e1-4bd7-86e9-86731b5c5b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token id: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "MAX_TOKENS = 128\n",
    "\n",
    "EOS_ID = llm.token_eos()\n",
    "print(\"EOS token id:\", EOS_ID)\n",
    "\n",
    "def ban_eos_logits_processor(input_ids, logits):\n",
    "    \"\"\"\n",
    "    llama-cpp-python logits_processor hook:\n",
    "    - input_ids: array of token ids so far\n",
    "    - logits: array of logits for next token\n",
    "    We set EOS logit to -inf so generation won't end early.\n",
    "    \"\"\"\n",
    "    logits[EOS_ID] = -1e10\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "322582e6-7a44-4a1b-bfba-15d373d7d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Write a continuous technical explanation of energy efficiency in AI inference. \"\n",
    "    \"Do not conclude. Keep expanding with details, examples, and tradeoffs.\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b65f8-9793-4381-82b8-c2ac292fa157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once():\n",
    "    process = psutil.Process()\n",
    "    mem_before = process.memory_info().rss / 1e6\n",
    "\n",
    "    t0 = time.time()\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        top_k=0, \n",
    "        repeat_penalty=1.0,\n",
    "        logits_processor=[ban_eos_logits_processor],\n",
    "    )\n",
    "    t1 = time.time()\n",
    "\n",
    "    mem_after = process.memory_info().rss / 1e6\n",
    "\n",
    "    text = out[\"choices\"][0][\"text\"]\n",
    "    n = out[\"usage\"][\"completion_tokens\"]\n",
    "    elapsed = t1 - t0\n",
    "    tps = n / elapsed if elapsed > 0 else float(\"inf\")\n",
    "\n",
    "    return {\n",
    "        \"tokens\": n,\n",
    "        \"seconds\": elapsed,\n",
    "        \"toks_per_sec\": tps,\n",
    "        \"mem_delta_mb\": mem_after - mem_before,\n",
    "        \"text_tail\": text[-120:],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b140a25-93a0-4cb5-a9b5-dd8eeadd76ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: tokens=128  time=2.96s  tok/s=43.24\n",
      "Run 2: tokens=128  time=2.69s  tok/s=47.64\n",
      "Run 3: tokens=128  time=2.44s  tok/s=52.42\n",
      "\n",
      "Token counts: [128, 128, 128]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(3):\n",
    "    r = run_once()\n",
    "    results.append(r)\n",
    "    print(f\"Run {i+1}: tokens={r['tokens']}  time={r['seconds']:.2f}s  tok/s={r['toks_per_sec']:.2f}\")\n",
    "\n",
    "print(\"\\nToken counts:\", [r[\"tokens\"] for r in results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153b321-a77d-4ddb-af1b-6ea8c5f09c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18639d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "green-attention-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
